"""
ãƒãƒƒãƒ†ãƒªãƒ¼äº¤æ›å®Ÿç¸¾é›†è¨ˆã‚¢ãƒ—ãƒª
PTä¼æ¥­(user_company)æ¯ã«ã€user_nameã¨è‡ªè»¢è»Šãƒ¡ãƒ¼ã‚«ãƒ¼åˆ¥ã®é›†è¨ˆã‚’è¡Œã„ã¾ã™ã€‚
"""
import streamlit as st
import pandas as pd
import plotly.express as px
from typing import Dict, List, Tuple
import io
import os
import zipfile

try:
    import snowflake.connector
    from snowflake.connector.pandas_tools import write_pandas
    SNOWFLAKE_AVAILABLE = True
except ImportError:
    SNOWFLAKE_AVAILABLE = False

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="ãƒãƒƒãƒ†ãƒªãƒ¼äº¤æ›å®Ÿç¸¾é›†è¨ˆ",
    page_icon="ğŸ”‹",
    layout="wide"
)

# èªè¨¼æ©Ÿèƒ½ï¼ˆãƒ‘ãƒ–ãƒªãƒƒã‚¯ãƒ‡ãƒ—ãƒ­ã‚¤æ™‚ã¯å¸¸ã«æœ‰åŠ¹ï¼‰
# ãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºæ™‚ã®ã¿ç„¡åŠ¹åŒ–ã™ã‚‹å ´åˆã¯ç’°å¢ƒå¤‰æ•° DISABLE_AUTH=true ã‚’è¨­å®š
ENABLE_AUTH = os.getenv("DISABLE_AUTH", "false").lower() != "true"

if ENABLE_AUTH:
    try:
        from auth import check_password, logout, get_authenticated_user
        
        if not check_password():
            st.stop()
        
        # ãƒ­ã‚°ã‚¢ã‚¦ãƒˆãƒœã‚¿ãƒ³ã‚’ã‚µã‚¤ãƒ‰ãƒãƒ¼ã«è¿½åŠ 
        with st.sidebar:
            st.markdown("---")
            st.markdown(f"ğŸ‘¤ ãƒ­ã‚°ã‚¤ãƒ³ä¸­: **{get_authenticated_user()}**")
            if st.button("ğŸšª ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ"):
                logout()
    except Exception as e:
        st.error(f"ğŸ” èªè¨¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¨ãƒ©ãƒ¼: {e}")
        st.error("Secretsã« [passwords] ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„")
        st.code("""
[passwords]
admin = "ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãƒãƒƒã‚·ãƒ¥å€¤"
        """)
        st.stop()

@st.cache_data(show_spinner=False)
def load_excel_data(file_path: str) -> pd.DataFrame:
    """Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    try:
        df = pd.read_excel(file_path)
        return df
    except Exception as e:
        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None

@st.cache_data(show_spinner=False)
def load_excel_from_uploaded_file(uploaded_file) -> pd.DataFrame:
    """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
    try:
        df = pd.read_excel(uploaded_file)
        # Eåˆ—ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹4ï¼‰ã¨Våˆ—ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹21ï¼‰ã®åˆ—åã‚’ä¿å­˜
        if len(df.columns) > 4:
            df.attrs['e_column_name'] = df.columns[4]  # Eåˆ—
        if len(df.columns) > 21:
            df.attrs['v_column_name'] = df.columns[21]  # Våˆ—ï¼ˆuser_company(æ‰€å±)ï¼‰
        return df
    except Exception as e:
        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def detect_duplicates(df: pd.DataFrame) -> pd.DataFrame:
    """
    å‰å¾Œ1æ™‚é–“ã§åŒã˜è»Šä¸¡ç•ªå·ï¼ˆcodeï¼‰ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’é‡è¤‡ã¨ã—ã¦æ¤œå‡º
    
    Args:
        df: å…ƒãƒ‡ãƒ¼ã‚¿ï¼ˆdo_dateåˆ—ã¨codeåˆ—ãŒå¿…è¦ï¼‰
    
    Returns:
        é‡è¤‡ãƒ•ãƒ©ã‚°åˆ—ï¼ˆis_duplicateï¼‰ã‚’è¿½åŠ ã—ãŸDataFrame
    """
    df = df.copy()
    
    # é‡è¤‡ãƒ•ãƒ©ã‚°ã‚’åˆæœŸåŒ–
    df['is_duplicate'] = False
    
    # do_dateåˆ—ã‚’datetimeå‹ã«å¤‰æ›
    if 'do_date' not in df.columns:
        return df
    
    # codeåˆ—ã®å­˜åœ¨ç¢ºèª
    if 'code' not in df.columns:
        return df
    
    # do_dateåˆ—ã‚’datetimeå‹ã«å¤‰æ›ï¼ˆã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–ï¼‰
    df['do_date'] = pd.to_datetime(df['do_date'], errors='coerce')
    
    # è»Šä¸¡ç•ªå·ãŒç©ºã§ãªã„ã€ã‹ã¤æ—¥æ™‚ãŒæœ‰åŠ¹ãªãƒ¬ã‚³ãƒ¼ãƒ‰ã®ã¿å‡¦ç†
    valid_mask = df['code'].notna() & df['do_date'].notna()
    
    if not valid_mask.any():
        return df
    
    # è»Šä¸¡ç•ªå·ã¨æ—¥æ™‚ã§ã‚½ãƒ¼ãƒˆ
    df_sorted = df.sort_values(['code', 'do_date'])
    
    # å‰ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã¨ã®å·®åˆ†ã‚’è¨ˆç®—ï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰
    df_sorted['prev_code'] = df_sorted['code'].shift(1)
    df_sorted['prev_date'] = df_sorted['do_date'].shift(1)
    
    # åŒã˜è»Šä¸¡ç•ªå·ã§ã€å‰ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã¨ã®æ™‚é–“å·®ãŒ1æ™‚é–“ä»¥å†…ã®å ´åˆã¯é‡è¤‡
    df_sorted['time_diff'] = df_sorted['do_date'] - df_sorted['prev_date']
    df_sorted['is_duplicate'] = (
        (df_sorted['code'] == df_sorted['prev_code']) & 
        (df_sorted['time_diff'] <= pd.Timedelta(hours=1))
    )
    
    # å…ƒã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹é †ã«æˆ»ã™
    df.loc[df_sorted.index, 'is_duplicate'] = df_sorted['is_duplicate']
    
    return df

def upload_to_snowflake(df: pd.DataFrame, connection_params: dict, table_name: str) -> bool:
    """
    DataFrameã‚’Snowflakeã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    
    Args:
        df: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹DataFrame
        connection_params: Snowflakeæ¥ç¶šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        table_name: ãƒ†ãƒ¼ãƒ–ãƒ«å
    
    Returns:
        æˆåŠŸã—ãŸã‹ã©ã†ã‹
    """
    if not SNOWFLAKE_AVAILABLE:
        st.error("âŒ Snowflakeãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return False
    
    conn = None
    try:
        # Snowflakeã«æ¥ç¶š
        conn = snowflake.connector.connect(**connection_params)

        # ã‚«ãƒ©ãƒ åã‚’Snowflakeç”¨ã«ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        df_clean = df.copy()
        df_clean.columns = [
            col.replace(' ', '_')
               .replace('(', '_')
               .replace(')', '_')
               .replace('-', '_')
               .replace('.', '_')
            for col in df_clean.columns
        ]

        # ä¸€æ™‚åˆ—ã‚’å‰Šé™¤
        temp_cols = ['is_duplicate', 'åŸºæº–åˆ¤å®š', 'prev_code', 'prev_date', 'time_diff']
        df_clean = df_clean.drop(columns=[col for col in temp_cols if col in df_clean.columns], errors='ignore')

        # write_pandasã‚’ä½¿ç”¨ã—ã¦é«˜é€Ÿã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        success, nchunks, nrows, _ = write_pandas(
            conn=conn,
            df=df_clean,
            table_name=table_name.upper(),
            auto_create_table=True,
            overwrite=True,
            quote_identifiers=False
        )

        return success

    except Exception as e:
        st.error(f"âŒ Snowflakeã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
        return False
    finally:
        if conn is not None:
            conn.close()

def is_self_exchange(df: pd.DataFrame, row_index: int) -> bool:
    """
    Eåˆ—ã¨Våˆ—ã‚’å‚ç…§ã—ã¦ã€è‡ªç¤¾äº¤æ›åˆ†ã‹ã©ã†ã‹ã‚’åˆ¤å®š
    
    Args:
        df: DataFrameï¼ˆEåˆ—ã¨Våˆ—ã®åˆ—åãŒattrsã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ï¼‰
        row_index: åˆ¤å®šã™ã‚‹è¡Œã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    
    Returns:
        bool: è‡ªç¤¾äº¤æ›åˆ†ã®å ´åˆTrue
    """
    # Eåˆ—ã¨Våˆ—ã®åˆ—åã‚’å–å¾—
    e_col_name = df.attrs.get('e_column_name', None)
    v_col_name = df.attrs.get('v_column_name', 'user_company(æ‰€å±)')
    
    if e_col_name is None or e_col_name not in df.columns:
        return False
    
    # è‡ªç¤¾äº¤æ›åˆ†ã®çµ„ã¿åˆã‚ã›å®šç¾©
    self_exchange_mapping = {
        'ãƒˆãƒ¨ã‚¿ãƒ¢ãƒ“ãƒªãƒ†ã‚£æ±äº¬æ ªå¼ä¼šç¤¾': 'TMT',
        'æ±Ÿãƒå³¶é›»é‰„æ ªå¼ä¼šç¤¾': 'æ±Ÿãƒé›»',
        'ãƒ¢ãƒ“ãƒªãƒ†ã‚£ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æ ªå¼ä¼šç¤¾': 'MPF',
        'æ±æ€¥ãƒã‚¹æ ªå¼ä¼šç¤¾': 'æ±æ€¥ãƒã‚¹'
    }
    
    # å¯¾è±¡PTä¼æ¥­ã®ãƒªã‚¹ãƒˆ
    target_pt_companies = ['TMT', 'æ±Ÿãƒé›»', 'MPF', 'æ±æ€¥ãƒã‚¹']
    
    # è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
    row = df.iloc[row_index]
    e_value = row.get(e_col_name, None)
    v_value = row.get(v_col_name, None)
    
    # Eåˆ—ã¨Våˆ—ã®å€¤ãŒè‡ªç¤¾äº¤æ›åˆ†ã®çµ„ã¿åˆã‚ã›ã‹ãƒã‚§ãƒƒã‚¯
    if pd.notna(e_value) and pd.notna(v_value):
        e_str = str(e_value).strip()
        v_str = str(v_value).strip()
        
        # PTä¼æ¥­ãŒå¯¾è±¡ä¼æ¥­ã§ã€ã‹ã¤Eåˆ—ã¨Våˆ—ã®çµ„ã¿åˆã‚ã›ãŒè‡ªç¤¾äº¤æ›åˆ†ã®å ´åˆ
        if v_str in target_pt_companies:
            if e_str in self_exchange_mapping:
                if self_exchange_mapping[e_str] == v_str:
                    return True
    
    return False

def check_battery_standard(row):
    """
    ãƒãƒƒãƒ†ãƒªãƒ¼æ®‹é‡ãŒåŸºæº–å¤–ã‹ã©ã†ã‹ã‚’åˆ¤å®š
    
    åŸºæº–:
    - Panasonic: 25%ä»¥ä¸ŠãŒåŸºæº–å¤–
    - YAMAHA: 70%ä»¥ä¸ŠãŒåŸºæº–å¤–
    - DBS: 50%ä»¥ä¸ŠãŒåŸºæº–å¤–ï¼ˆãŸã ã—100%ã¯åŸºæº–å†…ï¼‰
    - glafit: 50%ä»¥ä¸ŠãŒåŸºæº–å¤–
    - ã‚·ãƒŠãƒãƒ³ã‚µã‚¤ã‚¯ãƒ«: 40%ä»¥ä¸ŠãŒåŸºæº–å¤–
    
    Returns:
        str: 'åŸºæº–å†…' or 'åŸºæº–å¤–'
    """
    maker = row['è‡ªè»¢è»Šãƒ¡ãƒ¼ã‚«ãƒ¼å']
    battery = row['battery_remaining']
    
    if pd.isna(battery):
        return None
    
    if maker == 'Panasonic':
        return 'åŸºæº–å¤–' if battery >= 25 else 'åŸºæº–å†…'
    elif maker == 'YAMAHA':
        return 'åŸºæº–å¤–' if battery >= 70 else 'åŸºæº–å†…'
    elif maker == 'DBS':
        if battery == 100:
            return 'åŸºæº–å†…'
        return 'åŸºæº–å¤–' if battery >= 50 else 'åŸºæº–å†…'
    elif maker == 'glafit':
        return 'åŸºæº–å¤–' if battery >= 50 else 'åŸºæº–å†…'
    elif maker == 'ã‚·ãƒŠãƒãƒ³ã‚µã‚¤ã‚¯ãƒ«':
        return 'åŸºæº–å¤–' if battery >= 40 else 'åŸºæº–å†…'
    else:
        return None

def aggregate_by_company_and_maker(df: pd.DataFrame) -> Tuple[Dict[str, pd.DataFrame], pd.DataFrame]:
    """
    PTä¼æ¥­æ¯ã«ã€user_nameã¨è‡ªè»¢è»Šãƒ¡ãƒ¼ã‚«ãƒ¼åˆ¥ã®é›†è¨ˆã‚’è¡Œã†
    åŸºæº–å†…/åŸºæº–å¤–ã€é‡è¤‡é™¤å¤–ã‚‚å«ã‚ã¦é›†è¨ˆ
    è‡ªç¤¾äº¤æ›åˆ†ï¼ˆEåˆ—ã¨Våˆ—ã®ç‰¹å®šçµ„ã¿åˆã‚ã›ï¼‰ã¯é™¤å¤–ã—ã€åˆ¥é€”è¿”ã™

    Returns:
        tuple: (é›†è¨ˆçµæœDict, è‡ªç¤¾äº¤æ›åˆ†DataFrame)
            - é›†è¨ˆçµæœDict: PTä¼æ¥­åã‚’ã‚­ãƒ¼ã€é›†è¨ˆçµæœDataFrameã‚’å€¤ã¨ã™ã‚‹è¾æ›¸
            - è‡ªç¤¾äº¤æ›åˆ†DataFrame: è‡ªç¤¾äº¤æ›åˆ†ã®ãƒ¬ã‚³ãƒ¼ãƒ‰
    """
    # Våˆ—ã®åˆ—åã‚’å‹•çš„ã«å–å¾—
    company_col = df.attrs.get('v_column_name', 'user_company(æ‰€å±)')
    if company_col not in df.columns:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åˆ—åã«ã€Œæ‰€å±ã€ã‚’å«ã‚€åˆ—ã‚’æ¢ã™
        company_cols = [col for col in df.columns if 'æ‰€å±' in str(col) or 'company' in str(col).lower()]
        if company_cols:
            company_col = company_cols[0]
        else:
            raise KeyError(f"PTä¼æ¥­åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åˆ—: {list(df.columns)}")

    user_col = 'user_name'
    maker_col = 'è‡ªè»¢è»Šãƒ¡ãƒ¼ã‚«ãƒ¼å'
    
    # é‡è¤‡æ¤œå‡ºã‚’å®Ÿè¡Œ
    df_with_standard = detect_duplicates(df)
    
    # attrsã‚’ç¶™æ‰¿ï¼ˆEåˆ—ã¨Våˆ—ã®åˆ—åæƒ…å ±ï¼‰
    if hasattr(df, 'attrs'):
        df_with_standard.attrs.update(df.attrs)
    
    # åŸºæº–åˆ¤å®šåˆ—ã‚’è¿½åŠ 
    df_with_standard['åŸºæº–åˆ¤å®š'] = df_with_standard.apply(check_battery_standard, axis=1)
    
    # è‡ªç¤¾äº¤æ›åˆ†ã‚’åˆ¤å®šï¼ˆis_self_exchangeåˆ—ã‚’è¿½åŠ ï¼‰
    # ã‚ˆã‚ŠåŠ¹ç‡çš„ã«ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦å‡¦ç†
    df_with_standard['is_self_exchange'] = False
    
    # Eåˆ—ã¨Våˆ—ã®åˆ—åã‚’å–å¾—
    e_col_name = df_with_standard.attrs.get('e_column_name', None)
    v_col_name = df_with_standard.attrs.get('v_column_name', 'user_company(æ‰€å±)')
    
    if e_col_name and e_col_name in df_with_standard.columns:
        # è‡ªç¤¾äº¤æ›åˆ†ã®çµ„ã¿åˆã‚ã›å®šç¾©
        self_exchange_mapping = {
            'ãƒˆãƒ¨ã‚¿ãƒ¢ãƒ“ãƒªãƒ†ã‚£æ±äº¬æ ªå¼ä¼šç¤¾': 'TMT',
            'æ±Ÿãƒå³¶é›»é‰„æ ªå¼ä¼šç¤¾': 'æ±Ÿãƒé›»',
            'ãƒ¢ãƒ“ãƒªãƒ†ã‚£ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ æ ªå¼ä¼šç¤¾': 'MPF',
            'æ±æ€¥ãƒã‚¹æ ªå¼ä¼šç¤¾': 'æ±æ€¥ãƒã‚¹'
        }
        
        # å¯¾è±¡PTä¼æ¥­ã®ãƒªã‚¹ãƒˆ
        target_pt_companies = ['TMT', 'æ±Ÿãƒé›»', 'MPF', 'æ±æ€¥ãƒã‚¹']
        
        # ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã—ã¦åˆ¤å®š
        e_values = df_with_standard[e_col_name].astype(str).str.strip()
        v_values = df_with_standard[v_col_name].astype(str).str.strip()
        
        # æ¡ä»¶: Våˆ—ãŒå¯¾è±¡PTä¼æ¥­ã§ã€ã‹ã¤Eåˆ—ã¨Våˆ—ã®çµ„ã¿åˆã‚ã›ãŒè‡ªç¤¾äº¤æ›åˆ†
        mask = v_values.isin(target_pt_companies) & e_values.isin(self_exchange_mapping.keys())
        for e_str, v_expected in self_exchange_mapping.items():
            df_with_standard.loc[mask & (e_values == e_str) & (v_values == v_expected), 'is_self_exchange'] = True
    
    # è‡ªç¤¾äº¤æ›åˆ†ã‚’åˆ†é›¢
    self_exchange_df = df_with_standard[df_with_standard['is_self_exchange'] == True].copy()
    df_for_aggregation = df_with_standard[df_with_standard['is_self_exchange'] == False].copy()
    
    # PTä¼æ¥­æ¯ã«é›†è¨ˆï¼ˆè‡ªç¤¾äº¤æ›åˆ†ã‚’é™¤å¤–ã—ãŸãƒ‡ãƒ¼ã‚¿ã§é›†è¨ˆï¼‰
    aggregated_data = {}
    
    companies = df_for_aggregation[company_col].dropna().unique()
    
    for i, company in enumerate(companies):
        # è©²å½“ä¼æ¥­ã®ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆè‡ªç¤¾äº¤æ›åˆ†ã‚’é™¤å¤–ï¼‰
        company_df = df_for_aggregation[df_for_aggregation[company_col] == company]
        
        # å„ãƒ¡ãƒ¼ã‚«ãƒ¼ã®çµæœã‚’æ ¼ç´ã™ã‚‹è¾æ›¸
        result_data = []
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼æ¯ã«é›†è¨ˆ
        for user in company_df[user_col].dropna().unique():
            user_df = company_df[company_df[user_col] == user]
            row_data = {'user_name': user}
            
            # é‡è¤‡ã‚’é™¤å¤–ã—ãŸãƒ‡ãƒ¼ã‚¿
            user_df_no_dup = user_df[user_df['is_duplicate'] == False]
            # é‡è¤‡ãƒ‡ãƒ¼ã‚¿
            user_df_dup = user_df[user_df['is_duplicate'] == True]
            
            # å„ãƒ¡ãƒ¼ã‚«ãƒ¼ã«ã¤ã„ã¦ã€åŸºæº–å†…/åŸºæº–å¤–ã‚’é›†è¨ˆ
            makers = ['Panasonic', 'YAMAHA', 'DBS', 'glafit', 'ã‚·ãƒŠãƒãƒ³ã‚µã‚¤ã‚¯ãƒ«', 'KUROAD']
            total = 0
            total_duplicates = 0
            
            for maker in makers:
                # é‡è¤‡é™¤å¤–ãƒ‡ãƒ¼ã‚¿ã§é›†è¨ˆï¼ˆé‡è¤‡ã¯å«ã¾ãªã„ï¼‰
                maker_df = user_df_no_dup[user_df_no_dup[maker_col] == maker]
                # é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã®ä»¶æ•°ï¼ˆå‚è€ƒå€¤ï¼‰
                maker_dup_count = len(user_df_dup[user_df_dup[maker_col] == maker])
                
                # åŸºæº–å†…ã®ä»¶æ•°ï¼ˆé‡è¤‡é™¤å¤–å¾Œï¼‰
                kijun_nai = len(maker_df[maker_df['åŸºæº–åˆ¤å®š'] == 'åŸºæº–å†…'])
                # åŸºæº–å¤–ã®ä»¶æ•°ï¼ˆé‡è¤‡é™¤å¤–å¾Œï¼‰
                kijun_gai = len(maker_df[maker_df['åŸºæº–åˆ¤å®š'] == 'åŸºæº–å¤–'])
                # åˆè¨ˆï¼ˆé‡è¤‡é™¤å¤–å¾Œã€åŸºæº–åˆ¤å®šãŒNoneã®å ´åˆã‚‚å«ã‚€ï¼‰
                maker_total = len(maker_df)
                
                # æ¤œè¨¼: åŸºæº–å†… + åŸºæº–å¤– = åˆè¨ˆ (KUROADãªã©åŸºæº–åˆ¤å®šãŒãªã„ã‚‚ã®ã‚’é™¤ã)
                # maker_total = kijun_nai + kijun_gai + (åŸºæº–åˆ¤å®šãªã—)
                
                row_data[f'{maker}_åŸºæº–å†…'] = kijun_nai
                row_data[f'{maker}_åŸºæº–å¤–'] = kijun_gai
                row_data[f'{maker}_åˆè¨ˆ'] = maker_total
                row_data[f'{maker}_é‡è¤‡é™¤å¤–æ•°'] = maker_dup_count
                
                total += maker_total
                total_duplicates += maker_dup_count
            
            row_data['ç·åˆè¨ˆ'] = total
            row_data['ç·é‡è¤‡é™¤å¤–æ•°'] = total_duplicates
            result_data.append(row_data)
        
        # DataFrameã«å¤‰æ›
        result_df = pd.DataFrame(result_data)
        
        # åˆè¨ˆè¡Œã‚’è¿½åŠ 
        total_row = {'user_name': 'åˆè¨ˆ'}
        for col in result_df.columns:
            if col != 'user_name':
                total_row[col] = result_df[col].sum()
        
        result_df = pd.concat([result_df, pd.DataFrame([total_row])], ignore_index=True)
        
        # åˆ—ã®é †åºã‚’æ•´ç†
        ordered_columns = ['user_name']
        for maker in makers:
            if f'{maker}_åŸºæº–å†…' in result_df.columns:
                ordered_columns.extend([
                    f'{maker}_åŸºæº–å†…', 
                    f'{maker}_åŸºæº–å¤–', 
                    f'{maker}_åˆè¨ˆ',
                    f'{maker}_é‡è¤‡é™¤å¤–æ•°'
                ])
        ordered_columns.extend(['ç·åˆè¨ˆ', 'ç·é‡è¤‡é™¤å¤–æ•°'])
        
        # å­˜åœ¨ã™ã‚‹åˆ—ã®ã¿ã‚’é¸æŠ
        existing_columns = [col for col in ordered_columns if col in result_df.columns]
        result_df = result_df[existing_columns]
        
        aggregated_data[company] = result_df
    
    return aggregated_data, self_exchange_df

def main():
    st.title("ğŸ”‹ ãƒãƒƒãƒ†ãƒªãƒ¼äº¤æ›å®Ÿç¸¾é›†è¨ˆã‚¢ãƒ—ãƒª")
    st.markdown("---")
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼šãƒ•ã‚¡ã‚¤ãƒ«é¸æŠ
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
        uploaded_file = st.file_uploader(
            "Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
            type=['xlsx', 'xls'],
            help="ãƒãƒƒãƒ†ãƒªãƒ¼äº¤æ›å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„"
        )
        
        # Snowflakeè¨­å®š
        if SNOWFLAKE_AVAILABLE:
            st.markdown("---")
            with st.expander("â˜ï¸ Snowflakeè‡ªå‹•è»¢é€è¨­å®š", expanded=False):
                st.markdown("ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ™‚ã«è‡ªå‹•çš„ã«Snowflakeã¸è»¢é€ã—ã¾ã™")
                
                enable_snowflake = st.checkbox("Snowflakeè‡ªå‹•è»¢é€ã‚’æœ‰åŠ¹åŒ–", value=False)
                
                if enable_snowflake:
                    sf_account = st.text_input("Account", help="ä¾‹: abc12345.ap-northeast-1.aws")
                    sf_user = st.text_input("User")
                    sf_password = st.text_input("Password", type="password")
                    sf_warehouse = st.text_input("Warehouse", value="COMPUTE_WH")
                    sf_database = st.text_input("Database")
                    sf_schema = st.text_input("Schema", value="PUBLIC")
                    sf_table = st.text_input("Table Name", value="BATTERY_EXCHANGE_RAW")
                    
                    # æ¥ç¶šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¿å­˜
                    if all([sf_account, sf_user, sf_password, sf_warehouse, sf_database, sf_schema, sf_table]):
                        st.session_state['snowflake_params'] = {
                            'account': sf_account,
                            'user': sf_user,
                            'password': sf_password,
                            'warehouse': sf_warehouse,
                            'database': sf_database,
                            'schema': sf_schema
                        }
                        st.session_state['snowflake_table'] = sf_table
                        st.session_state['snowflake_enabled'] = True
                    else:
                        st.session_state['snowflake_enabled'] = False
                else:
                    st.session_state['snowflake_enabled'] = False
        
        # ãƒãƒ¼ã‚¸ãƒ§ãƒ³æƒ…å ±ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        st.markdown("---")
        st.caption("Version: 2026-02-03-v11 (åˆ—åã®å‹•çš„å–å¾—å¯¾å¿œ)")
    
    # ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢
    if uploaded_file is not None:
        # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
        with st.spinner("ğŸ“‚ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
            df = load_excel_from_uploaded_file(uploaded_file)
        
        if df is not None:
            st.success(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {len(df):,}è¡Œã®ãƒ‡ãƒ¼ã‚¿")
            
            # Snowflakeã¸ã®è‡ªå‹•è»¢é€
            if st.session_state.get('snowflake_enabled', False):
                if 'snowflake_uploaded' not in st.session_state or st.session_state.get('current_file') != uploaded_file.name:
                    with st.spinner("â˜ï¸ Snowflakeã¸ãƒ‡ãƒ¼ã‚¿ã‚’è»¢é€ä¸­..."):
                        success = upload_to_snowflake(
                            df,
                            st.session_state['snowflake_params'],
                            st.session_state['snowflake_table']
                        )
                        
                        if success:
                            st.success(f"âœ… Snowflakeã¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: {st.session_state['snowflake_table']}")
                            st.session_state['snowflake_uploaded'] = True
                            st.session_state['current_file'] = uploaded_file.name
                        else:
                            st.error("âŒ Snowflakeã¸ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã«å¤±æ•—ã—ã¾ã—ãŸ")
                else:
                    st.info(f"â„¹ï¸ ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯æ—¢ã«Snowflakeã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã§ã™ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«: {st.session_state['snowflake_table']}ï¼‰")
                
            # ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
            with st.expander("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆæœ€åˆã®10è¡Œï¼‰"):
                st.dataframe(df.head(10))
            
            # é›†è¨ˆå®Ÿè¡Œãƒœã‚¿ãƒ³
            if st.button("ğŸ”„ é›†è¨ˆå®Ÿè¡Œ", type="primary", use_container_width=True):
                progress_bar = st.progress(0, text="é›†è¨ˆã‚’é–‹å§‹ã—ã¦ã„ã¾ã™...")
                status_text = st.empty()
                
                try:
                    status_text.text("ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æä¸­...")
                    progress_bar.progress(10, text="é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œå‡ºä¸­...")

                    # PTä¼æ¥­ã®ãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆVåˆ—ã®åˆ—åã‚’å‹•çš„ã«å–å¾—ï¼‰
                    company_col = df.attrs.get('v_column_name', 'user_company(æ‰€å±)')
                    if company_col not in df.columns:
                        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åˆ—åã«ã€Œæ‰€å±ã€ã‚’å«ã‚€åˆ—ã‚’æ¢ã™
                        company_cols = [col for col in df.columns if 'æ‰€å±' in str(col) or 'company' in str(col).lower()]
                        if company_cols:
                            company_col = company_cols[0]
                        else:
                            raise KeyError(f"PTä¼æ¥­åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚Våˆ—ï¼ˆ22åˆ—ç›®ï¼‰ã«æ‰€å±æƒ…å ±ãŒã‚ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚ç¾åœ¨ã®åˆ—: {list(df.columns[:25])}")

                    companies = df[company_col].dropna().unique()
                    total_companies = len(companies)

                    # åˆ—åã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜
                    st.session_state['company_col'] = company_col
                    
                    status_text.text(f"ğŸ” é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã‚’æ¤œå‡ºä¸­...ï¼ˆ{len(df):,}è¡Œï¼‰")
                    progress_bar.progress(30, text="é‡è¤‡ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œä¸­...")
                    
                    # é›†è¨ˆå®Ÿè¡Œï¼ˆé‡è¤‡æ¤œå‡ºã‚’å«ã‚€ã€è‡ªç¤¾äº¤æ›åˆ†ã‚’é™¤å¤–ï¼‰
                    aggregated_data, self_exchange_df = aggregate_by_company_and_maker(df)
                    
                    progress_bar.progress(90, text="é›†è¨ˆçµæœã‚’æº–å‚™ä¸­...")
                    st.session_state['aggregated_data'] = aggregated_data
                    st.session_state['self_exchange_df'] = self_exchange_df
                    
                    progress_bar.progress(100, text="å®Œäº†ï¼")
                    status_text.empty()
                    progress_bar.empty()
                    
                    self_exchange_count = len(self_exchange_df) if not self_exchange_df.empty else 0
                    success_msg = f"âœ… é›†è¨ˆå®Œäº†ï¼{len(aggregated_data)}ç¤¾ã®ãƒ‡ãƒ¼ã‚¿ã‚’é›†è¨ˆã—ã¾ã—ãŸ"
                    if self_exchange_count > 0:
                        success_msg += f"ï¼ˆè‡ªç¤¾äº¤æ›åˆ†: {self_exchange_count:,}ä»¶ã‚’é™¤å¤–ï¼‰"
                    st.success(success_msg)
                    st.balloons()
                    
                except Exception as e:
                    progress_bar.empty()
                    status_text.empty()
                    st.error(f"âŒ é›†è¨ˆã‚¨ãƒ©ãƒ¼: {e}")
                    import traceback
                    with st.expander("è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±"):
                        st.code(traceback.format_exc())
            
            # é›†è¨ˆçµæœã®è¡¨ç¤º
            if 'aggregated_data' in st.session_state:
                aggregated_data = st.session_state['aggregated_data']
            
                st.markdown("---")
                st.header("ğŸ“ˆ é›†è¨ˆçµæœ")
            
                # å…¨ä¼æ¥­ã®ãƒ‡ãƒ¼ã‚¿ã‚’1ã¤ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›
                st.subheader("å…¨PTä¼æ¥­ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
                
                st.info("ğŸ’¡ å…¨PTä¼æ¥­ã®é›†è¨ˆçµæœã¨ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™")
                st.warning("âš ï¸ ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚’å«ã‚€ãŸã‚ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãŒå¤§ãããªã‚Šã¾ã™")
                
                if st.button("ğŸ“¦ å…¨ä¼æ¥­ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’æº–å‚™", key="prepare_all_excel"):
                    with st.spinner(f"å…¨{len(aggregated_data)}ç¤¾ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ZIPåŒ–ä¸­..."):
                        # ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
                        zip_buffer = io.BytesIO()
                        
                        # ç”Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ä¸€æ™‚åˆ—ã‚’å‰Šé™¤
                        df_clean = df.copy()
                        temp_cols = ['is_duplicate', 'åŸºæº–åˆ¤å®š', 'prev_code', 'prev_date', 'time_diff', 'is_self_exchange']
                        df_clean = df_clean.drop(columns=[col for col in temp_cols if col in df_clean.columns], errors='ignore')
                        
                        # è‡ªç¤¾äº¤æ›åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
                        self_exchange_clean = None
                        if 'self_exchange_df' in st.session_state and not st.session_state['self_exchange_df'].empty:
                            self_exchange_clean = st.session_state['self_exchange_df'].copy()
                            self_exchange_clean = self_exchange_clean.drop(columns=[col for col in temp_cols if col in self_exchange_clean.columns], errors='ignore')
                        
                        with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                            progress_bar = st.progress(0)
                            total = len(aggregated_data) + 1  # ä¼æ¥­ã”ã¨ã®ãƒ•ã‚¡ã‚¤ãƒ« + å…¨ä¼æ¥­ã¾ã¨ã‚ãƒ•ã‚¡ã‚¤ãƒ«
                            
                            # å…¨ä¼æ¥­ã®é›†è¨ˆçµæœã‚’ã¾ã¨ã‚ãŸExcelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
                            all_companies_excel = io.BytesIO()
                            with pd.ExcelWriter(all_companies_excel, engine='openpyxl') as writer:
                                # å…¨ä¼æ¥­ã®é›†è¨ˆçµæœã‚’1ã¤ã®ã‚·ãƒ¼ãƒˆã«çµ±åˆ
                                all_companies_data = []
                                
                                for company, data in aggregated_data.items():
                                    # å„ä¼æ¥­ã®ãƒ‡ãƒ¼ã‚¿ã«ä¼æ¥­ååˆ—ã‚’è¿½åŠ 
                                    company_with_name = data.copy()
                                    company_with_name.insert(0, 'PTä¼æ¥­å', company)
                                    all_companies_data.append(company_with_name)
                                
                                # å…¨ä¼æ¥­ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
                                combined_df = pd.concat(all_companies_data, ignore_index=True)
                                combined_df.to_excel(writer, sheet_name='å…¨PTä¼æ¥­é›†è¨ˆ', index=False)
                                
                                # è‡ªç¤¾äº¤æ›åˆ†ã‚·ãƒ¼ãƒˆã‚’è¿½åŠ 
                                if self_exchange_clean is not None and not self_exchange_clean.empty:
                                    self_exchange_clean.to_excel(writer, sheet_name='è‡ªç¤¾äº¤æ›åˆ†', index=False)
                            
                            all_companies_excel.seek(0)
                            zip_file.writestr(
                                "å…¨ä¼æ¥­_é›†è¨ˆçµæœ.xlsx",
                                all_companies_excel.getvalue()
                            )
                            progress_bar.progress(1 / total)
                            
                            # å„ä¼æ¥­ã”ã¨ã«1ã¤ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
                            # åˆ—åã‚’å–å¾—ï¼ˆã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰ã€ã¾ãŸã¯attrsã‹ã‚‰ï¼‰
                            download_company_col = st.session_state.get('company_col', df.attrs.get('v_column_name', 'user_company(æ‰€å±)'))

                            for idx, (company, data) in enumerate(aggregated_data.items()):
                                excel_buffer = io.BytesIO()

                                with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
                                    # é›†è¨ˆçµæœã‚·ãƒ¼ãƒˆ
                                    data.to_excel(writer, sheet_name='é›†è¨ˆçµæœ', index=False)

                                    # ç”Ÿãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆ
                                    company_raw = df_clean[df_clean[download_company_col] == company].copy()
                                    company_raw.to_excel(writer, sheet_name='ç”Ÿãƒ‡ãƒ¼ã‚¿', index=False)

                                    # è‡ªç¤¾äº¤æ›åˆ†ã‚·ãƒ¼ãƒˆï¼ˆè©²å½“ä¼æ¥­ã®ã¿ï¼‰
                                    if self_exchange_clean is not None and not self_exchange_clean.empty:
                                        company_self_exchange = self_exchange_clean[self_exchange_clean[download_company_col] == company].copy()
                                        if not company_self_exchange.empty:
                                            company_self_exchange.to_excel(writer, sheet_name='è‡ªç¤¾äº¤æ›åˆ†', index=False)
                                
                                # ZIPã«è¿½åŠ ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼‰
                                safe_company_name = company.replace('/', '_').replace('\\', '_')
                                zip_file.writestr(
                                    f"{safe_company_name}_é›†è¨ˆçµæœ_ç”Ÿãƒ‡ãƒ¼ã‚¿.xlsx",
                                    excel_buffer.getvalue()
                                )
                                
                                progress_bar.progress((idx + 2) / total)  # +2ã¯å…¨ä¼æ¥­ãƒ•ã‚¡ã‚¤ãƒ«åˆ†ã¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹èª¿æ•´
                            
                            progress_bar.empty()
                        
                        zip_buffer.seek(0)
                        st.session_state['all_excel_data'] = zip_buffer.getvalue()
                        st.session_state['all_excel_filename'] = "å…¨PTä¼æ¥­_é›†è¨ˆçµæœ_ç”Ÿãƒ‡ãƒ¼ã‚¿.zip"
                        st.session_state['all_excel_mime'] = "application/zip"
                        st.success(f"âœ… ZIPãƒ•ã‚¡ã‚¤ãƒ«ã®æº–å‚™å®Œäº†ï¼ï¼ˆå…¨ä¼æ¥­_é›†è¨ˆçµæœ.xlsx + {len(aggregated_data)}ç¤¾ã®Excelãƒ•ã‚¡ã‚¤ãƒ«ï¼‰")
                
                # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã‚’è¡¨ç¤º
                if 'all_excel_data' in st.session_state:
                    st.download_button(
                        label=f"ğŸ“¥ {st.session_state['all_excel_filename']} ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=st.session_state['all_excel_data'],
                        file_name=st.session_state['all_excel_filename'],
                        mime=st.session_state.get('all_excel_mime', 'application/zip'),
                        key="download_all_excel"
                    )
            
    else:
        # ãƒ•ã‚¡ã‚¤ãƒ«ãŒé¸æŠã•ã‚Œã¦ã„ãªã„å ´åˆ
        st.info("ğŸ‘ˆ ã‚µã‚¤ãƒ‰ãƒãƒ¼ã‹ã‚‰Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„")
        

if __name__ == "__main__":
    main()

